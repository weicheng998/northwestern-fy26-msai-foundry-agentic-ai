{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83d75bd8",
   "metadata": {},
   "source": [
    "# Lab 9: Tracing and Observability for AI Agents\n",
    "\n",
    "Build **production-ready observability** for your AI agents using OpenTelemetry and Azure Monitor.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "1. Understand **distributed tracing concepts** and why they matter for AI agents\n",
    "2. Configure **Azure Monitor** for production trace collection\n",
    "3. Instrument **OpenAI SDK calls** with automatic tracing\n",
    "4. Create **custom spans** for business logic\n",
    "5. Analyze traces in the **Foundry portal**\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "| Requirement | Setup |\n",
    "|------------|-------|\n",
    "| Labs 1-8 completed | Agent patterns, evaluations |\n",
    "| Python environment | Run `uv sync --all-extras --dev` from project root |\n",
    "| Azure CLI logged in | Run `az login` in terminal |\n",
    "| Azure AI Foundry Project | [Create at ai.azure.com](https://ai.azure.com) |\n",
    "| Application Insights | Connected to your Foundry project |\n",
    "\n",
    "## Conceptual Foundation: Distributed Tracing\n",
    "\n",
    "### What is a Trace?\n",
    "\n",
    "A **trace** represents the complete execution path of a single request through your system. In the context of AI agents:\n",
    "\n",
    "```\n",
    "User Query: \"What's the weather in Seattle and calculate a 20% tip on $85\"\n",
    "                                    â”‚\n",
    "                                    â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         TRACE: request_abc123                           â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚ SPAN: agent_session (root span)                     2.3s total  â”‚   â”‚\n",
    "â”‚  â”‚                                                                  â”‚   â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚\n",
    "â”‚  â”‚  â”‚ SPAN: planning                                    800ms    â”‚ â”‚   â”‚\n",
    "â”‚  â”‚  â”‚   â””â”€â”€ SPAN: chat.completions (auto-instrumented)  750ms    â”‚ â”‚   â”‚\n",
    "â”‚  â”‚  â”‚         attributes: model=gpt-4o, tokens.input=45          â”‚ â”‚   â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚\n",
    "â”‚  â”‚                                                                  â”‚   â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚\n",
    "â”‚  â”‚  â”‚ SPAN: tool_execution                              100ms    â”‚ â”‚   â”‚\n",
    "â”‚  â”‚  â”‚   â”œâ”€â”€ SPAN: tool_fetch_weather                     40ms    â”‚ â”‚   â”‚\n",
    "â”‚  â”‚  â”‚   â”‚     attributes: tool.args={location: Seattle}          â”‚ â”‚   â”‚\n",
    "â”‚  â”‚  â”‚   â””â”€â”€ SPAN: tool_calculate_tip                     50ms    â”‚ â”‚   â”‚\n",
    "â”‚  â”‚  â”‚         attributes: tool.args={bill: 85, pct: 20}          â”‚ â”‚   â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚\n",
    "â”‚  â”‚                                                                  â”‚   â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚\n",
    "â”‚  â”‚  â”‚ SPAN: response_generation                         1.2s     â”‚ â”‚   â”‚\n",
    "â”‚  â”‚  â”‚   â””â”€â”€ SPAN: chat.completions (auto-instrumented)  1.1s     â”‚ â”‚   â”‚\n",
    "â”‚  â”‚  â”‚         attributes: model=gpt-4o, tokens.output=120        â”‚ â”‚   â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚\n",
    "â”‚  â”‚                                                                  â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Key Terminology\n",
    "\n",
    "| Term | Definition | AI Agent Example |\n",
    "|------|------------|------------------|\n",
    "| **Trace** | A directed acyclic graph (DAG) of spans representing one request | Complete user query â†’ response flow |\n",
    "| **Span** | A single unit of work with start time, duration, and metadata | One LLM call, one tool execution |\n",
    "| **Parent-Child** | Spans have hierarchical relationships | `agent_session` â†’ `planning` â†’ `chat.completions` |\n",
    "| **Attributes** | Key-value metadata attached to spans | `model.name=\"gpt-4o\"`, `tokens.input=45` |\n",
    "| **Context Propagation** | Passing trace IDs across service boundaries | Ensures all spans link to same trace |\n",
    "\n",
    "### Why Traces Matter for AI Agents\n",
    "\n",
    "| Problem | Without Tracing | With Tracing |\n",
    "|---------|-----------------|--------------|\n",
    "| **Debugging** | \"The agent gave wrong answer\" | See exact prompt, model response, tool outputs at each step |\n",
    "| **Performance** | \"It's slow sometimes\" | Identify bottleneck: was it the LLM (2s) or tool call (50ms)? |\n",
    "| **Cost Analysis** | \"How much did that cost?\" | Sum `tokens.input` + `tokens.output` across all LLM spans |\n",
    "| **Error Diagnosis** | \"Something failed\" | Pinpoint exact span with exception, see preceding context |\n",
    "| **Behavior Audit** | \"Why did it do that?\" | Trace shows: LLM decided to call tools X, Y; got results; synthesized |\n",
    "\n",
    "## OpenTelemetry: The Standard\n",
    "\n",
    "**OpenTelemetry (OTel)** is the CNCF standard for observability. It provides:\n",
    "\n",
    "1. **Unified API**: Same code works with any backend (Azure Monitor, Jaeger, etc.)\n",
    "2. **Auto-Instrumentation**: Libraries like `opentelemetry-instrumentation-openai-v2` automatically create spans\n",
    "3. **Semantic Conventions**: Standardized attribute names (e.g., `gen_ai.request.model`)\n",
    "\n",
    "### OpenTelemetry Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         Your Application                                    â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\n",
    "â”‚   â”‚   Your Code     â”‚    â”‚  OpenAI SDK     â”‚    â”‚ Azure AI SDK    â”‚        â”‚\n",
    "â”‚   â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚        â”‚\n",
    "â”‚   â”‚ tracer.start_   â”‚    â”‚ (auto-instru-   â”‚    â”‚ (auto-instru-   â”‚        â”‚\n",
    "â”‚   â”‚ as_current_span â”‚    â”‚  mented)        â”‚    â”‚  mented)        â”‚        â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚\n",
    "â”‚            â”‚                      â”‚                      â”‚                  â”‚\n",
    "â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\n",
    "â”‚                                   â–¼                                         â”‚\n",
    "â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚\n",
    "â”‚                    â”‚      TracerProvider          â”‚                         â”‚\n",
    "â”‚                    â”‚  â€¢ Creates spans             â”‚                         â”‚\n",
    "â”‚                    â”‚  â€¢ Manages context           â”‚                         â”‚\n",
    "â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚\n",
    "â”‚                                   â”‚                                         â”‚\n",
    "â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚\n",
    "â”‚                    â”‚      SpanProcessor           â”‚                         â”‚\n",
    "â”‚                    â”‚  â€¢ BatchSpanProcessor        â”‚                         â”‚\n",
    "â”‚                    â”‚  â€¢ SimpleSpanProcessor       â”‚                         â”‚\n",
    "â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚\n",
    "â”‚                                   â”‚                                         â”‚\n",
    "â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚\n",
    "â”‚                    â”‚        Exporter              â”‚                         â”‚\n",
    "â”‚                    â”‚  â€¢ AzureMonitorExporter      â”‚                         â”‚\n",
    "â”‚                    â”‚  â€¢ OTLPSpanExporter          â”‚                         â”‚\n",
    "â”‚                    â”‚  â€¢ ConsoleSpanExporter       â”‚                         â”‚\n",
    "â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                    â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â–¼                               â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚   Azure Monitor   â”‚           â”‚   OTLP Endpoint   â”‚\n",
    "        â”‚ (App Insights)    â”‚           â”‚ (AI Toolkit, etc) â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Key Insight: One TracerProvider Per Process\n",
    "\n",
    "âš ï¸ **Important**: OpenTelemetry allows only **one `TracerProvider`** per Python process. This means:\n",
    "- You must choose Azure Monitor OR AI Toolkit (not both simultaneously)\n",
    "- To switch backends, restart your kernel/process\n",
    "- This is why we have separate notebooks for each backend\n",
    "\n",
    "## Environment Variables\n",
    "\n",
    "```bash\n",
    "# Required in .env file\n",
    "AZURE_AI_PROJECT_ENDPOINT=https://your-project.services.ai.azure.com/api/projects/your-project\n",
    "AZURE_AI_MODEL_DEPLOYMENT_NAME=gpt-4o\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d469fa",
   "metadata": {},
   "source": [
    "## Choosing a Tracing Backend\n",
    "\n",
    "This lab uses **Azure Monitor** (production-grade). For local-only development, see the separate notebook `lab9b_aitk_tracing.ipynb`.\n",
    "\n",
    "### Azure Monitor vs AI Toolkit: When to Use Each\n",
    "\n",
    "| Aspect | Azure Monitor (This Lab) | AI Toolkit (lab9b_aitk_tracing.ipynb) |\n",
    "|--------|--------------------------|--------------------------------------|\n",
    "| **Use Case** | Production, team collaboration | Local development, quick iteration |\n",
    "| **Setup** | Requires Azure subscription + App Insights | Just VS Code extension |\n",
    "| **Data Retention** | 90 days (configurable) | Session only (not persisted) |\n",
    "| **Query/Analysis** | KQL queries, dashboards, alerts | Visual inspection only |\n",
    "| **Cost** | Pay per GB ingested | Free |\n",
    "| **Latency** | 1-2 min to appear in portal | Instant in VS Code |\n",
    "| **Best For** | Debugging production issues, auditing | Rapid prototyping, learning |\n",
    "\n",
    "### Technical Difference\n",
    "\n",
    "Both use OpenTelemetry, but with different **Exporters**:\n",
    "\n",
    "```python\n",
    "# Azure Monitor: Uses Azure-specific exporter\n",
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "configure_azure_monitor(connection_string=\"InstrumentationKey=...\")\n",
    "\n",
    "# AI Toolkit: Uses generic OTLP HTTP exporter\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "exporter = OTLPSpanExporter(endpoint=\"http://localhost:4318/v1/traces\")\n",
    "```\n",
    "\n",
    "> ðŸ“š **Documentation**: \n",
    "> - [Trace and observe AI agents](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/trace-observe-agents)\n",
    "> - [AI Toolkit Tracing](https://code.visualstudio.com/docs/intelligentapps/tracing)\n",
    "\n",
    "# Part 1: Setup and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b251b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded /Users/pablo/Desktop/githubRepos/teaching/northwestern/northwestern-fy26-msai-foundry-agentic-ai/.env\n",
      "âœ… Endpoint: https://nw-fy-26.services.ai.azure.com/api/project...\n",
      "âœ… Model: gpt-4.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Ensure Azure CLI is in PATH\n",
    "for p in [\"/opt/homebrew/bin\", \"/usr/local/bin\"]:\n",
    "    if p not in os.environ.get(\"PATH\", \"\"):\n",
    "        os.environ[\"PATH\"] = p + \":\" + os.environ.get(\"PATH\", \"\")\n",
    "\n",
    "# Load .env\n",
    "env_path = Path(\"../.env\")\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "    print(f\"âœ… Loaded {env_path.resolve()}\")\n",
    "\n",
    "endpoint = os.getenv(\"AZURE_AI_PROJECT_ENDPOINT\")\n",
    "model = os.getenv(\"AZURE_AI_MODEL_DEPLOYMENT_NAME\", \"gpt-4o\")\n",
    "\n",
    "print(f\"âœ… Endpoint: {endpoint[:50]}...\" if endpoint else \"âŒ Missing AZURE_AI_PROJECT_ENDPOINT\")\n",
    "print(f\"âœ… Model: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f780b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Install tracing packages\n",
    "# Run in terminal: uv pip install azure-ai-projects azure-monitor-opentelemetry opentelemetry-instrumentation-openai-v2\n",
    "\n",
    "# Core imports\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# OpenTelemetry imports\n",
    "from opentelemetry import trace\n",
    "\n",
    "print(\"âœ… All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cd8a96",
   "metadata": {},
   "source": [
    "# Part 2: Enable Content Capture (Optional)\n",
    "\n",
    "By default, OpenTelemetry instrumentation does **not** capture the actual content of messages (for privacy). To see full inputs/outputs, enable content capture:\n",
    "\n",
    "> âš ï¸ **Security Warning**: Tracing can capture sensitive information (user inputs, model outputs, tool arguments). Use these practices to reduce risk:\n",
    "> - Don't store secrets, credentials, or tokens in prompts, tool arguments, or span attributes\n",
    "> - Redact or minimize personal data before it appears in telemetry\n",
    "> - Treat trace data as production telemetry with same access controls as logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f57fd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Content capture enabled\n",
      "   Trace data will include full message content (prompts and responses)\n",
      "\n",
      "âš ï¸  Remember to disable in production if content contains PII\n"
     ]
    }
   ],
   "source": [
    "# Enable content capture for development\n",
    "# Both environment variables work - the first is for Azure SDK, second for OpenTelemetry instrumentation\n",
    "os.environ[\"AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED\"] = \"true\"\n",
    "os.environ[\"OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT\"] = \"true\"\n",
    "\n",
    "print(\"âœ… Content capture enabled\")\n",
    "print(\"   Trace data will include full message content (prompts and responses)\")\n",
    "print(\"\\nâš ï¸  Remember to disable in production if content contains PII\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fab1f23",
   "metadata": {},
   "source": [
    "# Part 3: Azure Monitor Tracing (Recommended)\n",
    "\n",
    "For production AND development, send traces to **Azure Monitor Application Insights**. This enables:\n",
    "- Viewing traces in the Foundry portal\n",
    "- Long-term storage and analysis\n",
    "- Alerting on errors or latency\n",
    "- Correlation with other Azure services\n",
    "\n",
    "## Setup Steps\n",
    "\n",
    "1. Navigate to **Tracing** in the Foundry portal left navigation\n",
    "2. Create or connect an **Application Insights** resource\n",
    "3. The SDK retrieves the connection string automatically from your project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "373dfbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Application Insights connected\n",
      "   Connection string: InstrumentationKey=656c9f99-cd7f-48ce-849a-4e8aef4...\n",
      "\n",
      "âœ… Azure Monitor configured\n",
      "âœ… OpenAI SDK instrumented\n",
      "   All OpenAI calls will be automatically traced!\n",
      "   View traces in Foundry portal: Project â†’ Tracing\n"
     ]
    }
   ],
   "source": [
    "# Configure Azure Monitor tracing\n",
    "# This follows the official Microsoft Foundry documentation\n",
    "\n",
    "try:\n",
    "    from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "    from opentelemetry.instrumentation.openai_v2 import OpenAIInstrumentor\n",
    "    AZURE_MONITOR_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    AZURE_MONITOR_AVAILABLE = False\n",
    "    print(f\"âš ï¸ Missing packages: {e}\")\n",
    "    print(\"   Install with: uv pip install azure-monitor-opentelemetry opentelemetry-instrumentation-openai-v2\")\n",
    "\n",
    "if AZURE_MONITOR_AVAILABLE:\n",
    "    # Connect to your Foundry project and get Application Insights connection string\n",
    "    project_client = AIProjectClient(\n",
    "        endpoint=endpoint,\n",
    "        credential=DefaultAzureCredential(),\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Get Application Insights connection string from project\n",
    "        connection_string = project_client.telemetry.get_application_insights_connection_string()\n",
    "        print(f\"âœ… Application Insights connected\")\n",
    "        print(f\"   Connection string: {connection_string[:50]}...\")\n",
    "        \n",
    "        # Configure Azure Monitor (enables telemetry collection)\n",
    "        configure_azure_monitor(connection_string=connection_string)\n",
    "        \n",
    "        # Instrument the OpenAI SDK - THIS IS KEY!\n",
    "        # This automatically captures all OpenAI API calls as spans\n",
    "        OpenAIInstrumentor().instrument()\n",
    "        \n",
    "        print(\"\\nâœ… Azure Monitor configured\")\n",
    "        print(\"âœ… OpenAI SDK instrumented\")\n",
    "        print(\"   All OpenAI calls will be automatically traced!\")\n",
    "        print(\"   View traces in Foundry portal: Project â†’ Tracing\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not connect to Application Insights: {e}\")\n",
    "        print(\"   Make sure Application Insights is connected to your Foundry project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09f9b21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Testing tracing with chat completion...\n",
      "\n",
      "ðŸ“‹ Available model deployments:\n",
      "   - gpt-4.1: GlobalStandard\n",
      "   - text-embedding-3-small: Standard\n",
      "\n",
      "ðŸ”— AI Services base: https://nw-fy-26.services.ai.azure.com\n",
      "   Using model deployment: gpt-4.1\n",
      "âœ… Response received!\n",
      "\n",
      "ðŸ“ Response:\n",
      "Signals whisper truthâ€”  \n",
      "Logs, traces, metrics converge,  \n",
      "Clarity unfolds.\n",
      "\n",
      "âœ… Trace sent to Application Insights!\n",
      "   View in Foundry portal: Project â†’ Tracing\n"
     ]
    }
   ],
   "source": [
    "# Test tracing with a simple chat completion\n",
    "# This demonstrates how traces appear in Application Insights\n",
    "print(\"ðŸ” Testing tracing with chat completion...\\n\")\n",
    "\n",
    "from opentelemetry import trace\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "# Show available deployments\n",
    "print(\"ðŸ“‹ Available model deployments:\")\n",
    "deployments = list(project_client.deployments.list())\n",
    "for d in deployments:\n",
    "    print(f\"   - {d.name}: {d.get('sku', {}).get('name', 'N/A')}\")\n",
    "\n",
    "# The AI Services endpoint (without /api/projects/dev)\n",
    "ai_services_base = endpoint.split(\"/api/projects\")[0]\n",
    "print(f\"\\nðŸ”— AI Services base: {ai_services_base}\")\n",
    "print(f\"   Using model deployment: {model}\")\n",
    "\n",
    "# Create an Azure OpenAI client with token credential\n",
    "token_provider = get_bearer_token_provider(\n",
    "    DefaultAzureCredential(),\n",
    "    \"https://cognitiveservices.azure.com/.default\"\n",
    ")\n",
    "\n",
    "openai_client = AzureOpenAI(\n",
    "    azure_endpoint=ai_services_base,\n",
    "    api_version=\"2024-10-21\",  # Azure OpenAI API version\n",
    "    azure_ad_token_provider=token_provider\n",
    ")\n",
    "\n",
    "# Make a traced request\n",
    "with tracer.start_as_current_span(\"example-tracing\") as span:\n",
    "    span.set_attribute(\"user.query\", \"Write a haiku about observability\")\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,  # This is the deployment name\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Keep responses brief.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Write a haiku about observability.\"}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    assistant_response = response.choices[0].message.content\n",
    "    span.set_attribute(\"assistant.response\", assistant_response[:100] if assistant_response else \"\")\n",
    "    \n",
    "    print(f\"âœ… Response received!\")\n",
    "    print(f\"\\nðŸ“ Response:\\n{assistant_response}\")\n",
    "\n",
    "print(\"\\nâœ… Trace sent to Application Insights!\")\n",
    "print(\"   View in Foundry portal: Project â†’ Tracing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aa2726",
   "metadata": {},
   "source": [
    "# Part 4: Custom Spans for Business Logic\n",
    "\n",
    "Now that basic tracing works, let's capture **your own code** with custom spans. This is useful when you have business logic that calls multiple LLM operations.\n",
    "\n",
    "## Why Custom Spans?\n",
    "\n",
    "| Without Custom Spans | With Custom Spans |\n",
    "|---------------------|-------------------|\n",
    "| See only individual LLM calls | See the full operation that triggered them |\n",
    "| Hard to correlate related calls | Related calls grouped under parent span |\n",
    "| No business context | Custom attributes like `claims_count` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d492ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Custom traced function defined: assess_claims_with_context\n"
     ]
    }
   ],
   "source": [
    "# Get a tracer instance for custom spans\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "# Example: A function that assesses claims using chat completions\n",
    "def build_assessment_prompt(claim: str, context: str) -> str:\n",
    "    \"\"\"Build a prompt for claim verification.\"\"\"\n",
    "    return f\"\"\"Evidence: {context}\n",
    "\n",
    "Assess this claim based on the evidence. Output only 'True', 'False', or 'NEE' (not enough evidence).\n",
    "\n",
    "Claim: {claim}\n",
    "\n",
    "Assessment:\"\"\"\n",
    "\n",
    "@tracer.start_as_current_span(\"assess_claims_with_context\")\n",
    "def assess_claims_with_context(claims: list, contexts: list) -> list:\n",
    "    \"\"\"Assess multiple claims against their contexts with full tracing.\"\"\"\n",
    "    responses = []\n",
    "    \n",
    "    # Add custom attribute to track how many claims we're processing\n",
    "    current_span = trace.get_current_span()\n",
    "    current_span.set_attribute(\"operation.claims_count\", len(claims))\n",
    "    \n",
    "    # System instructions for claim assessment\n",
    "    system_message = \"You assess claims. Output only 'True', 'False', or 'NEE' (not enough evidence). No explanations.\"\n",
    "    \n",
    "    for i, (claim, context) in enumerate(zip(claims, contexts)):\n",
    "        # Create a nested span for each assessment\n",
    "        with tracer.start_as_current_span(f\"assess_claim_{i}\") as child_span:\n",
    "            child_span.set_attribute(\"claim.text\", claim[:50])\n",
    "            \n",
    "            response = openai_client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_message},\n",
    "                    {\"role\": \"user\", \"content\": build_assessment_prompt(claim, context)}\n",
    "                ],\n",
    "                temperature=0.0,  # Deterministic for assessment\n",
    "                max_tokens=10\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content.strip('., ').strip()\n",
    "            child_span.set_attribute(\"assessment.result\", result)\n",
    "            responses.append(result)\n",
    "    \n",
    "    return responses\n",
    "\n",
    "print(\"âœ… Custom traced function defined: assess_claims_with_context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35135baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Running claim assessment with tracing...\n",
      "\n",
      "âœ… Claim: 'The Earth orbits the Sun....' â†’ True\n",
      "âŒ Claim: 'Water boils at 50Â°C at sea level....' â†’ False\n",
      "\n",
      "ðŸ“Ž View the trace hierarchy in Foundry portal\n",
      "   You'll see: assess_claims_with_context â†’ multiple chat completion spans\n"
     ]
    }
   ],
   "source": [
    "# Test the custom traced function\n",
    "print(\"ðŸ” Running claim assessment with tracing...\\n\")\n",
    "\n",
    "claims = [\n",
    "    \"The Earth orbits the Sun.\",\n",
    "    \"Water boils at 50Â°C at sea level.\",\n",
    "]\n",
    "contexts = [\n",
    "    \"The Earth completes one orbit around the Sun every 365.25 days.\",\n",
    "    \"At standard atmospheric pressure, water boils at 100Â°C (212Â°F).\",\n",
    "]\n",
    "\n",
    "results = assess_claims_with_context(claims, contexts)\n",
    "\n",
    "for claim, result in zip(claims, results):\n",
    "    emoji = \"âœ…\" if result == \"True\" else \"âŒ\" if result == \"False\" else \"â“\"\n",
    "    print(f\"{emoji} Claim: '{claim[:40]}...' â†’ {result}\")\n",
    "\n",
    "print(\"\\nðŸ“Ž View the trace hierarchy in Foundry portal\")\n",
    "print(\"   You'll see: assess_claims_with_context â†’ multiple chat completion spans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "692fe2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Flushing traces to Azure Monitor...\n",
      "âœ… Traces flushed!\n",
      "   Check the Foundry portal in ~1-2 minutes for trace data\n"
     ]
    }
   ],
   "source": [
    "# Part 5: Flush traces before proceeding\n",
    "# Ensure all traces are exported to Azure Monitor\n",
    "print(\"â³ Flushing traces to Azure Monitor...\")\n",
    "\n",
    "# The Azure Monitor exporter typically batches and sends traces\n",
    "# Give it a moment to complete\n",
    "import time\n",
    "time.sleep(2)\n",
    "\n",
    "print(\"âœ… Traces flushed!\")\n",
    "print(\"   Check the Foundry portal in ~1-2 minutes for trace data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe075f3d",
   "metadata": {},
   "source": [
    "## Alternative: Console Tracing (for CI/CD)\n",
    "\n",
    "> â„¹ï¸ **Note**: If you don't have Azure Monitor access (e.g., CI/CD pipelines), you can output traces to console instead.\n",
    "\n",
    "```python\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "\n",
    "def setup_console_tracing():\n",
    "    \"\"\"Configure tracing to output to console (for testing/CI).\"\"\"\n",
    "    resource = Resource.create({\"service.name\": \"my-agent\"})\n",
    "    tracer_provider = TracerProvider(resource=resource)\n",
    "    tracer_provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))\n",
    "    trace.set_tracer_provider(tracer_provider)\n",
    "```\n",
    "\n",
    "Don't run this if you already configured Azure Monitor above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41b5fc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Setting up agent-style tracing with tool calling...\n",
      "   This demonstrates how to trace complex agent workflows\n"
     ]
    }
   ],
   "source": [
    "## Part 5 Continued: Agent-Style Tool Calling with Tracing\n",
    "# Now let's trace a more complex scenario: an agent that uses tools\n",
    "# This shows the full trace hierarchy: session â†’ LLM â†’ tool calls\n",
    "\n",
    "print(\"ðŸ¤– Setting up agent-style tracing with tool calling...\")\n",
    "print(\"   This demonstrates how to trace complex agent workflows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "657be575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tool executor defined: execute_tool(tool_name, args)\n"
     ]
    }
   ],
   "source": [
    "# Define tool implementations (these would be called when agent uses tools)\n",
    "def execute_tool(tool_name: str, args: dict) -> str:\n",
    "    \"\"\"Execute a tool and return the result.\"\"\"\n",
    "    if tool_name == \"fetch_weather\":\n",
    "        location = args.get(\"location\", \"Unknown\")\n",
    "        weather_data = {\n",
    "            \"Seattle\": \"14Â°C, Rainy\",\n",
    "            \"New York\": \"22Â°C, Sunny\",\n",
    "            \"London\": \"16Â°C, Cloudy\",\n",
    "        }\n",
    "        return weather_data.get(location, \"20Â°C, Unknown conditions\")\n",
    "    elif tool_name == \"calculate_tip\":\n",
    "        bill = args.get(\"bill_amount\", 0)\n",
    "        percent = args.get(\"tip_percent\", 18.0)\n",
    "        tip = bill * (percent / 100)\n",
    "        return f\"Tip: ${tip:.2f}, Total: ${bill + tip:.2f}\"\n",
    "    return \"Unknown tool\"\n",
    "\n",
    "print(\"âœ… Tool executor defined: execute_tool(tool_name, args)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b94887",
   "metadata": {},
   "source": [
    "## Tool Schema Definitions\n",
    "\n",
    "Define OpenAI function calling schemas so the model knows what tools are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f85d168c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Running agent-style query with tool calling...\n",
      "\n",
      "ðŸ“ž Tools called: ['fetch_weather', 'calculate_tip']\n",
      "\n",
      "âœ… Response received with tool calling enabled\n",
      "   Check Foundry Tracing for detailed span hierarchy\n",
      "\n",
      "ðŸ“Ž View trace in Foundry portal: Project â†’ Tracing\n",
      "   You'll see: agent_session â†’ LLM calls with tool definitions\n"
     ]
    }
   ],
   "source": [
    "# Execute agent-like behavior with tool calling using our traced client\n",
    "print(\"ðŸ¤– Running agent-style query with tool calling...\\n\")\n",
    "\n",
    "# Define tool schemas for OpenAI function calling\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"fetch_weather\",\n",
    "            \"description\": \"Fetches current weather for a location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\": \"string\", \"description\": \"City name\"}\n",
    "                },\n",
    "                \"required\": [\"location\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate_tip\",\n",
    "            \"description\": \"Calculates tip amount for a bill\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"bill_amount\": {\"type\": \"number\", \"description\": \"Total bill amount\"},\n",
    "                    \"tip_percent\": {\"type\": \"number\", \"description\": \"Tip percentage\", \"default\": 18.0}\n",
    "                },\n",
    "                \"required\": [\"bill_amount\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run a traced request with tools available\n",
    "with tracer.start_as_current_span(\"agent_session\") as span:\n",
    "    span.set_attribute(\"user.query\", \"What's the weather in Seattle?\")\n",
    "    span.set_attribute(\"agent.tools\", [\"fetch_weather\", \"calculate_tip\"])\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Use tools when appropriate.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What's the weather in Seattle and calculate a 20% tip on $85?\"}\n",
    "        ],\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "        temperature=0.0\n",
    "    )\n",
    "    \n",
    "    # Log tool calls if any\n",
    "    if response.choices[0].message.tool_calls:\n",
    "        tool_names = [tc.function.name for tc in response.choices[0].message.tool_calls]\n",
    "        span.set_attribute(\"agent.tool_calls\", str(tool_names))\n",
    "        print(f\"ðŸ“ž Tools called: {tool_names}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Response received with tool calling enabled\")\n",
    "    print(f\"   Check Foundry Tracing for detailed span hierarchy\")\n",
    "\n",
    "print(\"\\nðŸ“Ž View trace in Foundry portal: Project â†’ Tracing\")\n",
    "print(\"   You'll see: agent_session â†’ LLM calls with tool definitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e36fef5",
   "metadata": {},
   "source": [
    "# Part 6: View Traces in Microsoft Foundry Portal\n",
    "\n",
    "## Steps to View Your Traces\n",
    "\n",
    "1. Navigate to: [ai.azure.com](https://ai.azure.com)\n",
    "2. Select your project\n",
    "3. Click **Tracing** in the left navigation\n",
    "4. Filter by time range (last 15 minutes)\n",
    "5. Click on any trace to see span hierarchy\n",
    "\n",
    "## What You'll See\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Trace: example-tracing              Duration: ~2.3s      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  â”œâ”€â”€ chat.completions.create           1500ms            â”‚\n",
    "â”‚  â”‚   â””â”€â”€ attributes: model=gpt-4o                        â”‚\n",
    "â”‚  â”‚                                                        â”‚\n",
    "â”‚  â””â”€â”€ assess_claims_with_context         900ms            â”‚\n",
    "â”‚      â”œâ”€â”€ assess_claim_0                 450ms            â”‚\n",
    "â”‚      â””â”€â”€ assess_claim_1                 450ms            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "> ðŸ’¡ **Tip**: Allow 1-2 minutes for traces to appear in the portal after running your code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73db7a96",
   "metadata": {},
   "source": [
    "# Part 7: Local Tracing with AI Toolkit\n",
    "\n",
    "> âš ï¸ **Important**: You cannot use Azure Monitor AND AI Toolkit in the same kernel session. OpenTelemetry only allows one `TracerProvider` per process.\n",
    "\n",
    "For local development with AI Toolkit, use the dedicated notebook: **`lab9b_aitk_tracing.ipynb`**\n",
    "\n",
    "## Why a Separate Notebook?\n",
    "\n",
    "| Issue | Explanation |\n",
    "|-------|-------------|\n",
    "| **Single TracerProvider** | OTel SDK limitation - only one exporter target per process |\n",
    "| **Cannot Switch** | Once `configure_azure_monitor()` runs, traces go to Azure forever |\n",
    "| **Clean Slate** | The AITK notebook starts fresh, sends to `localhost:4318` |\n",
    "\n",
    "## Quick Reference: AI Toolkit Setup\n",
    "\n",
    "1. Install [AI Toolkit for VS Code](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio)\n",
    "2. Open **Tracing** in AI Toolkit sidebar\n",
    "3. Click **Start Collector** (must show \"Status: Running\")\n",
    "4. Open `notebooks/lab9b_aitk_tracing.ipynb`\n",
    "5. Run all cells â†’ Click **Refresh** in Tracing view\n",
    "\n",
    "> ðŸ“š **Documentation**: [Tracing in AI Toolkit](https://code.visualstudio.com/docs/intelligentapps/tracing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09106a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… AI Toolkit Tracing Configured!\n",
      "   Endpoint: http://localhost:4318/v1/traces\n",
      "   Service: lab9-ai-toolkit-demo\n",
      "\n",
      "ðŸ“‹ Next steps:\n",
      "   1. Run the code cells below to generate traces\n",
      "   2. Click 'Refresh' in AI Toolkit Tracing view\n",
      "   3. You should see your traces appear!\n"
     ]
    }
   ],
   "source": [
    "# This cell is intentionally left as documentation only.\n",
    "# DO NOT run AI Toolkit setup in this notebook - it will conflict with Azure Monitor.\n",
    "#\n",
    "# For AI Toolkit tracing, use: notebooks/lab9b_aitk_tracing.ipynb\n",
    "#\n",
    "# The key difference in setup:\n",
    "#\n",
    "# AZURE MONITOR (this notebook):\n",
    "#   from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "#   configure_azure_monitor(connection_string=\"InstrumentationKey=...\")\n",
    "#   OpenAIInstrumentor().instrument()\n",
    "#\n",
    "# AI TOOLKIT (lab9b_aitk_tracing.ipynb):\n",
    "#   from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "#   tracer_provider = TracerProvider(resource=Resource.create({\"service.name\": \"my-app\"}))\n",
    "#   tracer_provider.add_span_processor(BatchSpanProcessor(\n",
    "#       OTLPSpanExporter(endpoint=\"http://localhost:4318/v1/traces\")\n",
    "#   ))\n",
    "#   trace.set_tracer_provider(tracer_provider)\n",
    "#   OpenAIInstrumentor().instrument()\n",
    "\n",
    "print(\"â„¹ï¸  This notebook uses Azure Monitor for tracing.\")\n",
    "print(\"   For local AI Toolkit tracing, open: lab9b_aitk_tracing.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ffb3d1",
   "metadata": {},
   "source": [
    "## Complete Agent Session Example with Tracing\n",
    "\n",
    "The following example shows a **complete agent loop** with full tracing. Whether you use Azure Monitor or AI Toolkit, you'll see the entire agent workflow:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Trace: agent_conversation                                                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  â”œâ”€â”€ planning (LLM decides what tools to use)                              â”‚\n",
    "â”‚  â”‚   â””â”€â”€ chat.completions.create                                           â”‚\n",
    "â”‚  â”‚                                                                          â”‚\n",
    "â”‚  â”œâ”€â”€ tool_execution (agent calls tools)                                    â”‚\n",
    "â”‚  â”‚   â”œâ”€â”€ fetch_weather â†’ \"14Â°C, Rainy\"                                     â”‚\n",
    "â”‚  â”‚   â””â”€â”€ calculate_tip â†’ \"Tip: $17.00\"                                     â”‚\n",
    "â”‚  â”‚                                                                          â”‚\n",
    "â”‚  â””â”€â”€ response_generation (LLM synthesizes final answer)                    â”‚\n",
    "â”‚      â””â”€â”€ chat.completions.create                                           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "This is exactly what you'll see in AI Toolkit's Trace Viewer or Foundry's Tracing portal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "448d88a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Running Complete Agent Session with Tracing\n",
      "=======================================================\n",
      "\n",
      "ðŸ“ User Query: What's the weather in Seattle? Also, calculate a 20% tip on $85.\n",
      "\n",
      "ðŸ” Agent Steps:\n",
      "   ðŸ”§ Calling tool: fetch_weather({'location': 'Seattle'})\n",
      "      â†’ Result: 14Â°C, Rainy\n",
      "   ðŸ”§ Calling tool: calculate_tip({'bill_amount': 85, 'tip_percent': 20})\n",
      "      â†’ Result: Tip: $17.00, Total: $102.00\n",
      "\n",
      "=======================================================\n",
      "ðŸ’¬ Agent Response:\n",
      "The weather in Seattle is currently 14Â°C and rainy.\n",
      "\n",
      "A 20% tip on $85 is $17.00, making your total $102.00.\n",
      "\n",
      "âœ… Full trace captured!\n",
      "   View in: Foundry Portal â†’ Tracing  OR  AI Toolkit â†’ Tracing\n",
      "\n",
      "ðŸ“Š Trace Hierarchy:\n",
      "   agent_conversation\n",
      "   â”œâ”€â”€ planning (LLM + tool selection)\n",
      "   â”œâ”€â”€ tool_execution\n",
      "   â”‚   â”œâ”€â”€ tool_fetch_weather\n",
      "   â”‚   â””â”€â”€ tool_calculate_tip\n",
      "   â””â”€â”€ response_generation (final LLM call)\n"
     ]
    }
   ],
   "source": [
    "# Complete Agent Session with Full Tracing\n",
    "# This demonstrates the entire agent loop: plan â†’ execute tools â†’ respond\n",
    "# Works with both Azure Monitor AND AI Toolkit!\n",
    "\n",
    "import json\n",
    "from opentelemetry import trace\n",
    "\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "def run_traced_agent_session(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Run a complete agent session with full tracing.\n",
    "    \n",
    "    This shows exactly what happens in an agent:\n",
    "    1. Planning: LLM decides which tools to call\n",
    "    2. Tool Execution: Agent runs the tools\n",
    "    3. Response: LLM generates final answer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define our tools (same as before)\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"fetch_weather\",\n",
    "                \"description\": \"Get current weather for a city\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\"location\": {\"type\": \"string\"}},\n",
    "                    \"required\": [\"location\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\", \n",
    "            \"function\": {\n",
    "                \"name\": \"calculate_tip\",\n",
    "                \"description\": \"Calculate tip for a bill\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"bill_amount\": {\"type\": \"number\"},\n",
    "                        \"tip_percent\": {\"type\": \"number\", \"default\": 18}\n",
    "                    },\n",
    "                    \"required\": [\"bill_amount\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Tool implementations\n",
    "    def execute_tool(name: str, args: dict) -> str:\n",
    "        with tracer.start_as_current_span(f\"tool_{name}\") as tool_span:\n",
    "            tool_span.set_attribute(\"tool.name\", name)\n",
    "            tool_span.set_attribute(\"tool.arguments\", json.dumps(args))\n",
    "            \n",
    "            if name == \"fetch_weather\":\n",
    "                result = {\"Seattle\": \"14Â°C, Rainy\", \"NYC\": \"22Â°C, Sunny\"}.get(\n",
    "                    args.get(\"location\", \"\"), \"20Â°C, Clear\"\n",
    "                )\n",
    "            elif name == \"calculate_tip\":\n",
    "                bill = args.get(\"bill_amount\", 0)\n",
    "                pct = args.get(\"tip_percent\", 18)\n",
    "                tip = bill * (pct / 100)\n",
    "                result = f\"Tip: ${tip:.2f}, Total: ${bill + tip:.2f}\"\n",
    "            else:\n",
    "                result = \"Unknown tool\"\n",
    "            \n",
    "            tool_span.set_attribute(\"tool.result\", result)\n",
    "            return result\n",
    "    \n",
    "    # === THE AGENT LOOP (fully traced) ===\n",
    "    with tracer.start_as_current_span(\"agent_conversation\") as session_span:\n",
    "        session_span.set_attribute(\"user.query\", user_query)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Use tools when needed.\"},\n",
    "            {\"role\": \"user\", \"content\": user_query}\n",
    "        ]\n",
    "        \n",
    "        # STEP 1: Planning - LLM decides what tools to use\n",
    "        with tracer.start_as_current_span(\"planning\") as plan_span:\n",
    "            plan_span.set_attribute(\"step\", \"planning\")\n",
    "            \n",
    "            response = openai_client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                tools=tools,\n",
    "                tool_choice=\"auto\",\n",
    "                temperature=0\n",
    "            )\n",
    "            \n",
    "            assistant_msg = response.choices[0].message\n",
    "            tool_calls = assistant_msg.tool_calls or []\n",
    "            plan_span.set_attribute(\"tools_requested\", len(tool_calls))\n",
    "        \n",
    "        # STEP 2: Tool Execution - Run each tool the agent requested\n",
    "        if tool_calls:\n",
    "            with tracer.start_as_current_span(\"tool_execution\") as exec_span:\n",
    "                exec_span.set_attribute(\"step\", \"tool_execution\")\n",
    "                exec_span.set_attribute(\"tool_count\", len(tool_calls))\n",
    "                \n",
    "                # Add assistant message with tool calls\n",
    "                messages.append(assistant_msg)\n",
    "                \n",
    "                for tc in tool_calls:\n",
    "                    tool_name = tc.function.name\n",
    "                    tool_args = json.loads(tc.function.arguments)\n",
    "                    \n",
    "                    print(f\"   ðŸ”§ Calling tool: {tool_name}({tool_args})\")\n",
    "                    result = execute_tool(tool_name, tool_args)\n",
    "                    print(f\"      â†’ Result: {result}\")\n",
    "                    \n",
    "                    # Add tool result to conversation\n",
    "                    messages.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tc.id,\n",
    "                        \"content\": result\n",
    "                    })\n",
    "        \n",
    "        # STEP 3: Response Generation - LLM creates final answer\n",
    "        with tracer.start_as_current_span(\"response_generation\") as resp_span:\n",
    "            resp_span.set_attribute(\"step\", \"response_generation\")\n",
    "            \n",
    "            final_response = openai_client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            final_answer = final_response.choices[0].message.content\n",
    "            resp_span.set_attribute(\"response_length\", len(final_answer))\n",
    "        \n",
    "        session_span.set_attribute(\"agent.success\", True)\n",
    "        return final_answer\n",
    "\n",
    "\n",
    "# Run the traced agent session\n",
    "print(\"ðŸ¤– Running Complete Agent Session with Tracing\")\n",
    "print(\"=\" * 55)\n",
    "print()\n",
    "\n",
    "query = \"What's the weather in Seattle? Also, calculate a 20% tip on $85.\"\n",
    "print(f\"ðŸ“ User Query: {query}\")\n",
    "print()\n",
    "print(\"ðŸ” Agent Steps:\")\n",
    "\n",
    "result = run_traced_agent_session(query)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 55)\n",
    "print(f\"ðŸ’¬ Agent Response:\\n{result}\")\n",
    "print()\n",
    "print(\"âœ… Full trace captured!\")\n",
    "print(\"   View in: Foundry Portal â†’ Tracing  OR  AI Toolkit â†’ Tracing\")\n",
    "print()\n",
    "print(\"ðŸ“Š Trace Hierarchy:\")\n",
    "print(\"   agent_conversation\")\n",
    "print(\"   â”œâ”€â”€ planning (LLM + tool selection)\")  \n",
    "print(\"   â”œâ”€â”€ tool_execution\")\n",
    "print(\"   â”‚   â”œâ”€â”€ tool_fetch_weather\")\n",
    "print(\"   â”‚   â””â”€â”€ tool_calculate_tip\")\n",
    "print(\"   â””â”€â”€ response_generation (final LLM call)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "725253f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… AIAgentsInstrumentor enabled\n",
      "   All agent operations will be automatically traced\n",
      "   Includes: agent creation, message handling, tool calls, runs\n"
     ]
    }
   ],
   "source": [
    "# Enable automatic Agent SDK instrumentation\n",
    "# This instruments all Azure AI Agents operations automatically\n",
    "\n",
    "try:\n",
    "    from azure.ai.agents.telemetry import AIAgentsInstrumentor\n",
    "    \n",
    "    # Instrument the Azure AI Agents SDK\n",
    "    AIAgentsInstrumentor().instrument()\n",
    "    \n",
    "    print(\"âœ… AIAgentsInstrumentor enabled\")\n",
    "    print(\"   All agent operations will be automatically traced\")\n",
    "    print(\"   Includes: agent creation, message handling, tool calls, runs\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"â„¹ï¸  AIAgentsInstrumentor not available (optional)\")\n",
    "    print(\"   Install with: pip install azure-ai-projects[agents]\")\n",
    "    print(\"   Manual tracing with spans will still work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2ab08c",
   "metadata": {},
   "source": [
    "# Part 8: Advanced Trace Analysis & KQL Queries\n",
    "\n",
    "## Trace Hierarchy Visualization\n",
    "\n",
    "You can view detailed span hierarchies in the Foundry portal:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Trace: traced_agent_session                           Duration: 2.3s      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                             â”‚\n",
    "â”‚  â”œâ”€â”€ create_agent                                      150ms               â”‚\n",
    "â”‚  â”‚   â””â”€â”€ attributes: agent.name=AzureMonitorAgent                          â”‚\n",
    "â”‚  â”‚                                                                          â”‚\n",
    "â”‚  â”œâ”€â”€ llm_call (planning)                               800ms               â”‚\n",
    "â”‚  â”‚   â””â”€â”€ attributes: model=gpt-4o, tokens.input=45                         â”‚\n",
    "â”‚  â”‚                                                                          â”‚\n",
    "â”‚  â”œâ”€â”€ tool_call (calculate_tip)                         50ms                â”‚\n",
    "â”‚  â”‚   â””â”€â”€ attributes: tool.args={bill_amount: 85, tip_percent: 20}          â”‚\n",
    "â”‚  â”‚                                                                          â”‚\n",
    "â”‚  â””â”€â”€ llm_call (response)                               1200ms              â”‚\n",
    "â”‚      â””â”€â”€ attributes: model=gpt-4o, tokens.output=120                       â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Conversation Results\n",
    "\n",
    "A **Conversation** is the persistent context of an end-to-end dialogue history between a user and an agent. In the Foundry portal, you can view Conversation results including:\n",
    "\n",
    "- Conversation history details\n",
    "- Response information and tokens in a run\n",
    "- Ordered actions, run steps, and tool calls\n",
    "- Inputs and outputs between a user and an agent\n",
    "\n",
    "Search by Conversation ID, Response ID, or Trace ID to review specific conversations.\n",
    "\n",
    "## Azure Monitor Query (KQL)\n",
    "\n",
    "Use Kusto Query Language to analyze traces programmatically:\n",
    "\n",
    "```sql\n",
    "-- Find all traces from your service\n",
    "traces\n",
    "| where cloud_RoleName == \"lab9-azure-monitor-demo\"\n",
    "| order by timestamp desc\n",
    "| take 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a87d48e",
   "metadata": {},
   "source": [
    "# Part 9: Summary and Key Takeaways\n",
    "\n",
    "## Core Concepts Recap\n",
    "\n",
    "### 1. Traces and Spans\n",
    "\n",
    "```\n",
    "TRACE (one request end-to-end)\n",
    "â”‚\n",
    "â”œâ”€â”€ SPAN: agent_session (root)          â† Your custom span\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ SPAN: planning                  â† Your custom span\n",
    "â”‚   â”‚   â””â”€â”€ SPAN: chat.completions      â† Auto-instrumented by OpenAIInstrumentor\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ SPAN: tool_execution            â† Your custom span\n",
    "â”‚   â”‚   â”œâ”€â”€ SPAN: tool_weather          â† Your custom span\n",
    "â”‚   â”‚   â””â”€â”€ SPAN: tool_calculator       â† Your custom span\n",
    "â”‚   â”‚\n",
    "â”‚   â””â”€â”€ SPAN: response_generation       â† Your custom span\n",
    "â”‚       â””â”€â”€ SPAN: chat.completions      â† Auto-instrumented\n",
    "```\n",
    "\n",
    "**Key insight**: `OpenAIInstrumentor().instrument()` automatically creates spans for all OpenAI API calls. You add custom spans to provide business context.\n",
    "\n",
    "### 2. Two Tracing Backends\n",
    "\n",
    "| Backend | Notebook | Exporter | Use When |\n",
    "|---------|----------|----------|----------|\n",
    "| **Azure Monitor** | `lab9_tracing_observability.ipynb` (this) | `configure_azure_monitor()` | Production, team, persistence |\n",
    "| **AI Toolkit** | `lab9b_aitk_tracing.ipynb` | `OTLPSpanExporter(localhost:4318)` | Local dev, rapid iteration |\n",
    "\n",
    "âš ï¸ **Cannot mix**: One `TracerProvider` per process. Choose one per session.\n",
    "\n",
    "### 3. Auto-Instrumentation vs Custom Spans\n",
    "\n",
    "| Type | Code | Creates |\n",
    "|------|------|---------|\n",
    "| **Auto** | `OpenAIInstrumentor().instrument()` | Spans for every `chat.completions.create()` call |\n",
    "| **Auto** | `AIAgentsInstrumentor().instrument()` | Spans for Azure AI Agents SDK operations |\n",
    "| **Custom** | `with tracer.start_as_current_span(\"name\"):` | Your business logic spans |\n",
    "| **Custom** | `@tracer.start_as_current_span(\"name\")` | Decorator form |\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Setup Reference\n",
    "\n",
    "### Azure Monitor (Production)\n",
    "\n",
    "```python\n",
    "# 1. Get connection string from your Foundry project\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "from opentelemetry.instrumentation.openai_v2 import OpenAIInstrumentor\n",
    "\n",
    "project = AIProjectClient(endpoint=os.environ[\"AZURE_AI_PROJECT_ENDPOINT\"], \n",
    "                          credential=DefaultAzureCredential())\n",
    "conn_str = project.telemetry.get_application_insights_connection_string()\n",
    "\n",
    "# 2. Configure Azure Monitor\n",
    "configure_azure_monitor(connection_string=conn_str)\n",
    "\n",
    "# 3. Instrument OpenAI SDK (KEY STEP!)\n",
    "OpenAIInstrumentor().instrument()\n",
    "\n",
    "# 4. All OpenAI calls now traced automatically!\n",
    "```\n",
    "\n",
    "### AI Toolkit (Local Development)\n",
    "\n",
    "```python\n",
    "# See lab9b_aitk_tracing.ipynb for complete example\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "\n",
    "tracer_provider = TracerProvider(resource=Resource.create({\"service.name\": \"my-agent\"}))\n",
    "tracer_provider.add_span_processor(BatchSpanProcessor(\n",
    "    OTLPSpanExporter(endpoint=\"http://localhost:4318/v1/traces\")\n",
    "))\n",
    "trace.set_tracer_provider(tracer_provider)\n",
    "OpenAIInstrumentor().instrument()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Variables\n",
    "\n",
    "| Variable | Purpose |\n",
    "|----------|---------|\n",
    "| `OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT=true` | Capture prompt/response content |\n",
    "| `AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED=true` | Same as above (Azure SDK variant) |\n",
    "| `OTEL_SERVICE_NAME=my-service` | Identify your service in traces |\n",
    "\n",
    "---\n",
    "\n",
    "## Framework Integration Summary\n",
    "\n",
    "| Framework | Instrumentation | Package |\n",
    "|-----------|----------------|---------|\n",
    "| **OpenAI SDK** | `OpenAIInstrumentor().instrument()` | `opentelemetry-instrumentation-openai-v2` |\n",
    "| **Azure AI Agents** | `AIAgentsInstrumentor().instrument()` | `azure-ai-agents` |\n",
    "| **Semantic Kernel** | Built-in (automatic) | - |\n",
    "| **LangChain** | `AzureAIOpenTelemetryTracer` callback | `langchain-azure-ai` |\n",
    "| **Custom Code** | `tracer.start_as_current_span()` | `opentelemetry-sdk` |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Traces = Request Journey**: A trace captures everything that happens for one user request\n",
    "2. **Spans = Individual Operations**: Each LLM call, tool execution, or function is a span\n",
    "3. **Auto-Instrumentation is Key**: `OpenAIInstrumentor().instrument()` does most of the work\n",
    "4. **Custom Spans Add Context**: Wrap your business logic to see the full picture\n",
    "5. **Choose Backend Wisely**: Azure Monitor for prod, AI Toolkit for local dev\n",
    "6. **One Provider Per Process**: Cannot mix backends in same session\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "### Microsoft Documentation\n",
    "- [Trace and observe AI agents](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/trace-observe-agents)\n",
    "- [Tracing in AI Toolkit](https://code.visualstudio.com/docs/intelligentapps/tracing)\n",
    "\n",
    "### OpenTelemetry\n",
    "- [OpenTelemetry Python SDK](https://opentelemetry.io/docs/languages/python/)\n",
    "- [Semantic Conventions for GenAI](https://opentelemetry.io/docs/specs/semconv/gen-ai/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "northwestern-fy26-msai-foundry-agentic-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
